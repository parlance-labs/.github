# ğŸ‘‹ Hey there! We're Parlance Labs

We're a group of AI engineers led by [Hamel Husain](https://hamel.dev) who help companies figure out if their AI is actually working (spoiler: often it's not) and how to make it better.

## What we're all about

Here's what drives our approach:

- We build tools for systematic AI evaluation (because how else will you know if your AI is doing anything useful?)
- We're data-obsessed and have learned through [30+ AI implementations](https://hamel.dev/blog/posts/field-guide/) that focusing on tools over process kills AI projects
- We've developed practical methods for [error analysis](https://hamel.dev/blog/posts/evals/) and [avoiding metric overload](https://hamel.dev/blog/posts/llm-judge/) that actually move the needle

## We teach you how to fish

Here's something different about us: **we don't want you to have a long-term dependency on us**.

Our consultancy model is explicitly designed to make ourselves unnecessary. We don't chase recurring revenue or sell maintenance contracts - we think that creates perverse incentives.

Instead, we work alongside your team, transfer knowledge, and build capabilities so that when we leave, you have:
- The skills to evaluate your AI systems
- Processes for continuous improvement
- Independence from consultants (including us!)

When we succeed, you don't need us anymore. That's exactly how we want it.

## Find us around the internet

- ğŸŒ [parlance-labs.com](https://parlance-labs.com) - More about what we do
- ğŸ“ [hamel.dev](https://hamel.dev) - Hamel's blog with deeper technical dives
- ğŸ¦ [@HamelHusain](https://x.com/HamelHusain) - Hamel's Twitter/X where he posts about AI, data, and occasionally rants about LLM frameworks

